import transformers
import torch
import pandas as pd
import json
import numpy as np
from tqdm import tqdm
import os
import cn_clip.clip as clip
from cn_clip.clip import load_from_name
from PIL import Image
from opencc import OpenCC

# åˆå§‹åŒ–ç®€ç¹è½¬æ¢å™¨
cc = OpenCC('t2s')


def extract_news_elements(model_id, title, content):
    """
    ä»æ–°é—»æ–‡æœ¬ä¸­æå–å…³é”®è¦ç´ å¹¶ä¿å­˜åˆ° Excel æ–‡ä»¶
    :param model_id: æ¨¡å‹è·¯å¾„
    :param title: æ–°é—»æ ‡é¢˜
    :param content: æ–°é—»æ­£æ–‡
    :return: è§£æåçš„æ–°é—»è¦ç´ å­—å…¸
    """
    # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
    tokenizer = transformers.AutoTokenizer.from_pretrained(model_id)
    deepseek_model = transformers.AutoModelForCausalLM.from_pretrained(
        model_id,
        torch_dtype=torch.bfloat16,
        device_map="auto"
    )

    # è§†è§‰è¦ç´ æç¤º
    system_prompt = """
    ä½œä¸ºæ–°é—»è§†è§‰åˆ†æå¸ˆï¼Œéœ€ä»æ–°é—»å†…å®¹ä¸­æå–ä»¥ä¸‹7å¤§è¦ç´ ï¼Œä»¥JSONæ ¼å¼è¾“å‡ºï¼š
    è¦ç´ è§„èŒƒï¼ˆå¿…å¡«ï¼‰ï¼š
    1. æ ¸å¿ƒäº‹ä»¶ï¼ˆåè¯çŸ­è¯­ï¼Œä¸€å®šè¦å’Œæ ‡é¢˜ç´§å¯†ç›¸å…³ï¼Œ3-5ä¸ªï¼‰
    2. å…³é”®åŠ¨ä½œï¼ˆåŠ¨å®¾ç»“æ„ï¼Œ2-3ä¸ªï¼‰
    3. æ˜¾è‘—å®ä½“ï¼ˆå…·ä½“åç§°ï¼Œå…·ä½“çš„äººåå’Œç‰©å“åå’Œæœºæ„åä¸€å®šè¦ç›´æ¥æå–å‡ºæ¥ï¼Œä¸éœ€è¦åœ¨åé¢æ³¨æ˜æ˜¯äººåè¿˜æ˜¯æœºæ„åï¼‰
    4. åœºæ™¯ç‰¹å¾ï¼ˆç¯å¢ƒ/æ—¶é—´/è§†è§‰å…ƒç´ ï¼‰
    5. æƒ…æ„Ÿå†…å®¹(ä¸°å¯Œ)
    6. è§†è§‰éšå–»ï¼ˆè±¡å¾æ€§å…ƒç´ ï¼‰
    7. æ•°æ®ç»Ÿè®¡ä¿¡æ¯ï¼ˆæ–°é—»ä¸­çš„æ•°æ®å’Œç»Ÿè®¡ä¿¡æ¯ï¼‰
    è¾“å‡ºå¿…é¡»ä½¿ç”¨å¦‚ä¸‹æ ¼å¼ï¼š
    æ ¸å¿ƒäº‹ä»¶ï¼š[...] \n å…³é”®åŠ¨ä½œï¼š[...] \n æ˜¾è‘—å®ä½“ï¼š[...] \n åœºæ™¯ç‰¹å¾ï¼š[...] \n æƒ…æ„Ÿå†…å®¹ï¼š[...] \n è§†è§‰éšå–»ï¼š[...]\n æ•°æ®ç»Ÿè®¡ä¿¡æ¯ï¼š[...]
    """

    input_text = f"æ ‡é¢˜ï¼š{title}\næ­£æ–‡ï¼š{content}"
    messages = [{"role": "system", "content": system_prompt}, {"role": "user", "content": input_text}]

    # ç”Ÿæˆä¸è§£æ
    input_ids = tokenizer.apply_chat_template(messages, return_tensors="pt", add_generation_prompt=True).to(
        deepseek_model.device)
    outputs = deepseek_model.generate(input_ids, max_new_tokens=2048)
    raw_output = tokenizer.decode(outputs[0], skip_special_tokens=True)

    def parse_elements(text):
        elements = {
            "æ ¸å¿ƒäº‹ä»¶": [], "å…³é”®åŠ¨ä½œ": [], "æ˜¾è‘—å®ä½“": [],
            "åœºæ™¯ç‰¹å¾": [], "æƒ…æ„Ÿå€¾å‘": [], "è§†è§‰éšå–»": [], "æ•°æ®ç»Ÿè®¡ä¿¡æ¯": []
        }
        try:
            json_block = text.split("```json")[-1].split("```")[0].strip()
            parsed_json = json.loads(json_block)
            converted_parsed = {cc.convert(k): v for k, v in parsed_json.items()}
            return {k: converted_parsed.get(cc.convert(k), []) for k in elements}
        except:
            return elements

    parsed = parse_elements(raw_output)
    print(parsed)

    # ä¿å­˜åˆ° Excel
    excel_data = {
        "æ ‡é¢˜": [title], "æ­£æ–‡": [content],
        "æ ¸å¿ƒäº‹ä»¶": [parsed.get("æ ¸å¿ƒäº‹ä»¶", [])],
        "å…³é”®åŠ¨ä½œ": [parsed.get("å…³é”®åŠ¨ä½œ", [])],
        "æ˜¾è‘—å®ä½“": [parsed.get("æ˜¾è‘—å®ä½“", [])],
        "åœºæ™¯ç‰¹å¾": [parsed.get("åœºæ™¯ç‰¹å¾", [])],
        "æƒ…æ„Ÿå€¾å‘": [parsed.get("æƒ…æ„Ÿå€¾å‘", [])],
        "è§†è§‰éšå–»": [parsed.get("è§†è§‰éšå–»", [])],
        "æ•°æ®ç»Ÿè®¡ä¿¡æ¯": [parsed.get("æ•°æ®ç»Ÿè®¡ä¿¡æ¯", [])]
    }
    excel_path = "news_analysis.xlsx"
    df = pd.DataFrame(excel_data)
    df.to_excel(excel_path, index=False, engine='openpyxl')

    # æ¸…ç† DeepSeek æ¨¡å‹å ç”¨çš„ GPU å†…å­˜
    del deepseek_model
    torch.cuda.empty_cache()

    return parsed


def preprocess_image_features(model_name, image_dir, cache_dir):
    """
    é¢„å¤„ç†å›¾åƒç‰¹å¾å¹¶è¿›è¡Œç¼“å­˜
    :param model_name: æ¨¡å‹åç§°
    :param image_dir: å›¾åƒæ–‡ä»¶å¤¹è·¯å¾„
    :param cache_dir: ç¼“å­˜æ–‡ä»¶å¤¹è·¯å¾„
    :return: å›¾åƒç‰¹å¾å­—å…¸
    """
    device = "cuda" if torch.cuda.is_available() else "cpu"
    clip_model, clip_preprocess = load_from_name(model_name, device=device, download_root='./')
    clip_model.to(device)

    cache_file = os.path.join(cache_dir, "image_features.npy")
    if os.path.exists(cache_file):
        print(f"ä»ç¼“å­˜åŠ è½½å›¾ç‰‡åµŒå…¥: {cache_file}")
        return np.load(cache_file, allow_pickle=True).item()

    print(f"ğŸ›  é¢„å¤„ç†å›¾ç‰‡å¹¶ç”Ÿæˆç¼“å­˜: {cache_file}")
    image_features = {}
    for img_name in tqdm(os.listdir(image_dir), desc="é¢„å¤„ç†å›¾ç‰‡"):
        try:
            image = Image.open(os.path.join(image_dir, img_name)).convert("RGB")
            inputs = clip_preprocess(image).unsqueeze(0).to(device)
            with torch.no_grad():
                feat = clip_model.encode_image(inputs).cpu().numpy()
            image_features[img_name] = feat / np.linalg.norm(feat)
        except Exception as e:
            print(f"è·³è¿‡æ— æ•ˆå›¾ç‰‡ {img_name}: {str(e)}")
            continue
    np.save(cache_file, image_features)
    print(f"ç¼“å­˜å·²ä¿å­˜: {cache_file}")
    return image_features


def encode_text(text, text_feature_cache, model_name):
    """
    å¯¹æ–‡æœ¬è¿›è¡Œç¼–ç 
    :param text: å¾…ç¼–ç çš„æ–‡æœ¬
    :param text_feature_cache: æ–‡æœ¬ç‰¹å¾ç¼“å­˜å­—å…¸
    :param model_name: æ¨¡å‹åç§°
    :return: ç¼–ç åçš„æ–‡æœ¬ç‰¹å¾å‘é‡
    """
    if not text.strip():
        return np.zeros((1, 512))
    cache_key = hash(text)
    if cache_key in text_feature_cache:
        return text_feature_cache[cache_key]

    device = "cuda" if torch.cuda.is_available() else "cpu"
    clip_model, _ = load_from_name(model_name, device=device, download_root='./')
    clip_model.to(device)

    tokens = clip.tokenize([text]).to(device)
    with torch.no_grad():
        feat = clip_model.encode_text(tokens).cpu().numpy()
    normalized_feat = feat / np.linalg.norm(feat, axis=1, keepdims=True)
    text_feature_cache[cache_key] = normalized_feat
    return normalized_feat


def calculate_text_feature(news_row, fixed_weights, title, content, word_threshold, model_name):
    """
    è®¡ç®—æ–‡æœ¬ç‰¹å¾å‘é‡
    :param news_row: æ–°é—»è¦ç´ æ•°æ®è¡Œ
    :param fixed_weights: å›ºå®šæƒé‡å­—å…¸
    :param content: æ–°é—»æ­£æ–‡
    :param word_threshold: æ­£æ–‡é•¿åº¦é˜ˆå€¼
    :param model_name: æ¨¡å‹åç§°
    :return: æ–‡æœ¬ç‰¹å¾å‘é‡
    """
    text_feature_cache = {}
    content_length = len(content)
    if content_length < word_threshold:
        # æ­£æ–‡é•¿åº¦å°äºé˜ˆå€¼ï¼Œç›´æ¥ç”¨æ­£æ–‡åŒ¹é…å›¾ç‰‡

        text_feat = encode_text(title+content, text_feature_cache, model_name)
        print('æ­£åœ¨åŒ¹é…' + title+content)
    else:
        # æ­£å¸¸æµç¨‹ï¼šä½¿ç”¨å›ºå®šæƒé‡è®¡ç®—åŠ æƒæ–‡æœ¬ç‰¹å¾
        weighted_feat = np.zeros((1, 512))
        converted_news_row = {cc.convert(k): v for k, v in news_row.items()}
        for field, weight in fixed_weights.items():
            text = str(converted_news_row.get(cc.convert(field), ""))
            weighted_feat += weight * encode_text(text, text_feature_cache, model_name)
        text_feat = weighted_feat / np.linalg.norm(weighted_feat)  # å½’ä¸€åŒ–ç‰¹å¾å‘é‡
    return text_feat


def calculate_similarities(text_feat, image_features):
    """
    è®¡ç®—æ–‡æœ¬ä¸å›¾åƒçš„ç›¸ä¼¼åº¦
    :param text_feat: æ–‡æœ¬ç‰¹å¾å‘é‡
    :param image_features: å›¾åƒç‰¹å¾å­—å…¸
    :return: æŒ‰ç›¸ä¼¼åº¦é™åºæ’åˆ—çš„å›¾åƒåˆ—è¡¨
    """
    similarities = []
    for img_name, img_feat in image_features.items():
        sim = text_feat @ img_feat.T
        similarities.append((img_name, sim[0][0]))
    similarities.sort(key=lambda x: -x[1])  # æŒ‰ç›¸ä¼¼åº¦é™åºæ’åˆ—
    return similarities


def main():

    # æ¨¡å‹é…ç½®
    model_id = "/home/zjm/llm/DeepSeek-R1-Distill-Llama-8B-hf"
    title = """
    "â€œæ›¸ç”Ÿä¾˜å¯‚â€”æ–½æ°¸è¯ä¸ƒåå€‹å±•â€9æœˆ14æ—¥åœ¨è‡ºä¸­å¸‚æ¸¯å€è—è¡“ä¸­å¿ƒèˆ‰è¾¦é–‹å¹•å¼ã€‚
    """
    content = """
"â€œæ›¸ç”Ÿä¾˜å¯‚â€”æ–½æ°¸è¯ä¸ƒåå€‹å±•â€9æœˆ14æ—¥åœ¨è‡ºä¸­å¸‚æ¸¯å€è—è¡“ä¸­å¿ƒèˆ‰è¾¦é–‹å¹•å¼ã€‚å±•è¦½å…±åˆ†ç‚ºä¸ƒå€‹ä¸»é¡Œï¼ŒåŒ…å«â€œè‡ªæ›¸è‡ªè©±â€å‘ˆç¾æ–½æ°¸è¯ä»¥ç™½è©±è©©è¨˜éŒ„æ—¥å¸¸ï¼Œæ‰“ç ´åƒ…é™å¤å…¸è©©è©çš„æ›¸æ³•æ¡†æ¶ï¼Œå°‡æ›¸æ³•èˆ‡ç¾ä»£ç”Ÿæ´»çµåˆï¼›â€œèˆŠä½œå“å›é¡§â€å‘ˆç¾ä»–æ›¸æ³•é¢¨æ ¼çš„æ¼”è®Šï¼›â€œè‡¨å¸–â€å±•å‡ºä»–è‡¨æ‘¹å¤ä»£æ›¸å¸–çš„ä½œå“ï¼›â€œæ–°è©©æ›¸å¯«â€æ›¸å¯«ä¸ƒä½åŒ…æ‹¬æ•…å‹å’Œä»–å´‡æ‹œçš„è©©äººï¼Œå±•ç¾è©©çš„æ„å¢ƒï¼›â€œç´…æ¨“å¤¢â€å˜—è©¦ä»¥å°èªªæˆ–æ—¥è¨˜çš„å½¢å¼å®Œæˆæ›¸æ³•å‰µä½œï¼›â€œå°è¯â€å±•ç¤ºä¸åŒç­†èª¿æ›¸å¯«çš„å°è¯ä½œå“ï¼›â€œå¯¦é©—å‰µä½œâ€å±•ç¤ºåœ¨ç­†æ³•ã€å¸ƒå±€ã€å¢¨è‰²ã€è£è£±å’Œæ‹¼è²¼ä¸Šçš„å¯¦é©—å‰µä½œï¼Œæ¢ç´¢æ›¸æ³•æ–°è¡¨ç¾ã€‚åœ–ç‚ºæ–½æ°¸è¯èˆ‡ä½œå“ã€Šç—´ã€‹ã€‚  é¦™æ¸¯ä¸­é€šç¤¾åœ–ç‰‡

"""

    # æ­¥éª¤ 1ï¼šç”Ÿæˆæ–°é—»è¦ç´  Excel
    parsed = extract_news_elements(model_id, title, content)

    # é…ç½®å‚æ•°
    model_name = "ViT-B-16"
    cache_dir = "CNA_cache"
    image_dir = "/home/zjm/CNA_images"
    output_dir = "result"
    os.makedirs(cache_dir, exist_ok=True)
    os.makedirs(output_dir, exist_ok=True)

    # å®šä¹‰ Excel åˆ—å
    columns = ["æ ‡é¢˜", "æ­£æ–‡", "æ ¸å¿ƒäº‹ä»¶", "å…³é”®åŠ¨ä½œ",
               "æ˜¾è‘—å®ä½“", "åœºæ™¯ç‰¹å¾", "æƒ…æ„Ÿå€¾å‘", "è§†è§‰éšå–»", "æ•°æ®ç»Ÿè®¡ä¿¡æ¯"]

    # åŠ è½½æ–°é—»è¦ç´ æ•°æ®
    news_df = pd.read_excel("news_analysis.xlsx", sheet_name='Sheet1', nrows=1)
    if len(news_df) == 0:
        raise ValueError("Excel æ–‡ä»¶ä¸­æ— æ•°æ®")
    news_row = news_df[columns].iloc[0]

    # å›ºå®šæƒé‡é…ç½®
    fixed_weights = {
        "æ ‡é¢˜": 0.3,  # æ ‡é¢˜é€šå¸¸åŒ…å«æ ¸å¿ƒä¿¡æ¯ï¼Œæƒé‡è¾ƒé«˜
        "æ ¸å¿ƒäº‹ä»¶": 0.25,  # æ ¸å¿ƒäº‹ä»¶æ˜¯æ–°é—»çš„æ ¸å¿ƒè¯­ä¹‰å•å…ƒ
        "å…³é”®åŠ¨ä½œ": 0.05,  # å…³é”®åŠ¨ä½œæè¿°äº‹ä»¶åŠ¨æ€
        "æ˜¾è‘—å®ä½“": 0.25,  # æ˜¾è‘—å®ä½“ï¼ˆäººåã€æœºæ„åï¼‰æ˜¯é‡è¦åŒ¹é…çº¿ç´¢
        "åœºæ™¯ç‰¹å¾": 0.08,  # åœºæ™¯ç‰¹å¾ï¼ˆæ—¶é—´ã€åœ°ç‚¹ï¼‰è¾…åŠ©å®šä½
        "æƒ…æ„Ÿå€¾å‘": 0.03,  # æƒ…æ„Ÿå€¾å‘å¯¹è§†è§‰æ°›å›´æœ‰å½±å“ï¼Œä½†æƒé‡è¾ƒä½
        "è§†è§‰éšå–»": 0.02,  # è§†è§‰éšå–»è¾ƒæŠ½è±¡ï¼Œæƒé‡æœ€ä½
        "æ•°æ®ç»Ÿè®¡ä¿¡æ¯": 0.02,  # æ•°æ®ç»Ÿè®¡ä½œä¸ºè¡¥å……ä¿¡æ¯
    }
    # ç¡®ä¿æƒé‡å’Œä¸º 1
    total_weight = sum(fixed_weights.values())
    fixed_weights = {k: v / total_weight for k, v in fixed_weights.items()}
    print("ä½¿ç”¨å›ºå®šæƒé‡ï¼š", fixed_weights)

    # æ­¥éª¤ 2ï¼šåŠ è½½å›¾åƒç‰¹å¾
    image_features = preprocess_image_features(model_name, image_dir, cache_dir)

    # æ­¥éª¤ 3ï¼šè®¡ç®—æ–‡æœ¬ç‰¹å¾
    word_threshold = 250
    text_feat = calculate_text_feature(news_row, fixed_weights, title, content, word_threshold, model_name)

    # æ­¥éª¤ 4ï¼šè®¡ç®— Top åŒ¹é…å›¾åƒ
    similarities = calculate_similarities(text_feat, image_features)

    print("\nTop10 åŒ¹é…å›¾åƒ:")
    for i, (img_name, sim) in enumerate(similarities[:100], 1):
        print(f"{i}. {img_name} ç›¸ä¼¼åº¦: {sim:.4f}")


if __name__ == "__main__":
    main()
